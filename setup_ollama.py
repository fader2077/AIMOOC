"""
Ollama ç’°å¢ƒæª¢æŸ¥å’Œåˆå§‹åŒ–è…³æœ¬
"""
import subprocess
import sys
import time
import requests


def check_ollama_installed():
    """æª¢æŸ¥ Ollama æ˜¯å¦å·²å®‰è£"""
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        if result.returncode == 0:
            print(f"âœ… Ollama å·²å®‰è£: {result.stdout.strip()}")
            return True
        return False
    except (FileNotFoundError, subprocess.TimeoutExpired):
        print("âŒ Ollama æœªå®‰è£")
        print("è«‹è¨ªå• https://ollama.com ä¸‹è¼‰ä¸¦å®‰è£ Ollama")
        return False


def check_ollama_running():
    """æª¢æŸ¥ Ollama æœå‹™æ˜¯å¦é‹è¡Œ"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code == 200:
            print("âœ… Ollama æœå‹™æ­£åœ¨é‹è¡Œ")
            return True
        return False
    except requests.exceptions.RequestException:
        print("âŒ Ollama æœå‹™æœªé‹è¡Œ")
        return False


def start_ollama_service():
    """å•Ÿå‹• Ollama æœå‹™"""
    print("\nå˜—è©¦å•Ÿå‹• Ollama æœå‹™...")
    print("è«‹åœ¨å¦ä¸€å€‹çµ‚ç«¯çª—å£é‹è¡Œ: ollama serve")
    print("æˆ–åœ¨ Windows ä¸Šï¼ŒOllama æ‡‰è©²æœƒè‡ªå‹•å•Ÿå‹•")
    
    # ç­‰å¾…ç”¨æˆ¶å•Ÿå‹•æœå‹™
    print("\nç­‰å¾… Ollama æœå‹™å•Ÿå‹•...")
    for i in range(30):
        time.sleep(1)
        if check_ollama_running():
            return True
        if i % 5 == 0:
            print(f"  ç­‰å¾…ä¸­... ({i+1}/30 ç§’)")
    
    print("âŒ Ollama æœå‹™å•Ÿå‹•è¶…æ™‚")
    return False


def check_model_installed(model_name):
    """æª¢æŸ¥æŒ‡å®šæ¨¡å‹æ˜¯å¦å·²ä¸‹è¼‰"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json().get("models", [])
            model_names = [m["name"] for m in models]
            
            # æª¢æŸ¥ç²¾ç¢ºåŒ¹é…æˆ–åŒ…å«åŒ¹é…
            for installed_model in model_names:
                if model_name in installed_model or installed_model in model_name:
                    print(f"âœ… æ¨¡å‹ {model_name} å·²å®‰è£")
                    return True
            
            print(f"âŒ æ¨¡å‹ {model_name} æœªå®‰è£")
            return False
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥æ¨¡å‹: {str(e)}")
        return False


def pull_model(model_name):
    """ä¸‹è¼‰æ¨¡å‹"""
    print(f"\nğŸ“¥ é–‹å§‹ä¸‹è¼‰æ¨¡å‹: {model_name}")
    print(f"è«‹é‹è¡Œ: ollama pull {model_name}")
    print("é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜åˆ°å¹¾ååˆ†é˜ï¼Œå–æ±ºæ–¼æ¨¡å‹å¤§å°å’Œç¶²é€Ÿ...")
    
    try:
        process = subprocess.Popen(
            ["ollama", "pull", model_name],
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )
        
        # é¡¯ç¤ºä¸‹è¼‰é€²åº¦
        for line in process.stdout:
            print(f"  {line.strip()}")
        
        process.wait()
        
        if process.returncode == 0:
            print(f"âœ… æ¨¡å‹ {model_name} ä¸‹è¼‰å®Œæˆ")
            return True
        else:
            print(f"âŒ æ¨¡å‹ {model_name} ä¸‹è¼‰å¤±æ•—")
            return False
            
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰éç¨‹å‡ºéŒ¯: {str(e)}")
        return False


def setup_ollama():
    """å®Œæ•´çš„ Ollama è¨­ç½®æµç¨‹"""
    print("=" * 60)
    print("ğŸš€ Ollama ç’°å¢ƒæª¢æŸ¥å’Œè¨­ç½®")
    print("=" * 60)
    print()
    
    # 1. æª¢æŸ¥å®‰è£
    if not check_ollama_installed():
        print("\nè«‹å…ˆå®‰è£ Ollama:")
        print("  Windows: https://ollama.com/download/windows")
        print("  macOS: https://ollama.com/download/mac")
        print("  Linux: curl -fsSL https://ollama.com/install.sh | sh")
        sys.exit(1)
    
    print()
    
    # 2. æª¢æŸ¥æœå‹™
    if not check_ollama_running():
        if not start_ollama_service():
            print("\nè«‹æ‰‹å‹•å•Ÿå‹• Ollama:")
            print("  é‹è¡Œ: ollama serve")
            sys.exit(1)
    
    print()
    
    # 3. æª¢æŸ¥å¿…éœ€æ¨¡å‹
    import config
    
    required_models = set(config.OLLAMA_MODELS.values())
    print(f"ğŸ“‹ éœ€è¦çš„æ¨¡å‹: {', '.join(required_models)}")
    print()
    
    missing_models = []
    for model in required_models:
        if not check_model_installed(model):
            missing_models.append(model)
    
    # 4. ä¸‹è¼‰ç¼ºå¤±çš„æ¨¡å‹
    if missing_models:
        print(f"\néœ€è¦ä¸‹è¼‰ {len(missing_models)} å€‹æ¨¡å‹")
        print("æ¨è–¦æ¨¡å‹å¤§å°å’Œç”¨é€”:")
        print("  - llama3.1:8b  (~4.7GB) - é€šç”¨æ¨¡å‹ï¼Œé‚è¼¯æ¨ç†å¼·")
        print("  - gemma2:9b    (~5.5GB) - Google é–‹ç™¼ï¼Œå‰µæ„å¯«ä½œå¥½")
        print("  - qwen2.5:7b   (~4.4GB) - ä¸­æ–‡å„ªåŒ–ï¼Œæ•¸æ“šè™•ç†å¿«")
        print()
        
        choice = input("æ˜¯å¦è‡ªå‹•ä¸‹è¼‰æ‰€æœ‰ç¼ºå¤±çš„æ¨¡å‹? (y/n): ").lower()
        if choice == 'y':
            for model in missing_models:
                pull_model(model)
        else:
            print("\nè«‹æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹:")
            for model in missing_models:
                print(f"  ollama pull {model}")
            sys.exit(1)
    
    print()
    print("=" * 60)
    print("âœ… Ollama ç’°å¢ƒæº–å‚™å®Œæˆï¼")
    print("=" * 60)
    print()
    print("ç¾åœ¨å¯ä»¥é‹è¡Œ:")
    print("  python demo.py       # æ¼”ç¤ºæ¨¡å¼")
    print("  python test_ollama.py # æ¸¬è©¦ Ollama")
    print("  python app.py        # Web ä»‹é¢")
    print()


if __name__ == "__main__":
    setup_ollama()
